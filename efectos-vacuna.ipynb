{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Twitter Using snscrape\n",
    "<br>Package Github: https://github.com/JustAnotherArchivist/snscrape\n",
    "<br>This notebook will be using the development version of snscrape\n",
    "\n",
    "Article Read-Along: https://medium.com/better-programming/how-to-scrape-tweets-with-snscrape-90124ed006af\n",
    "\n",
    "### Author: Martin Beck\n",
    "\n",
    "<b>Dependencies: </b> \n",
    "- Your <b>Python</b> version must be <b>3.8</b> or higher. The development version of snscrape will not work with Python 3.7 or lower. You can download the latest Python version [here](https://www.python.org/downloads/).\n",
    "- <b>Development version of snscrape</b>, uncomment the pip install line in the below cell to pip install in the notebook if you don't already have it.\n",
    "- <b>Pandas</b>, the dataframes allows easy manipulation and indexing of data, this is more of a preference but is what I follow in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import snscrape.modules.twitter as sntwitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consulta por búsqueda de texto\n",
    "El siguiente código buscará tweets a través de los siguientes términos:\n",
    "\n",
    "* 'Dolor vacuna'\n",
    "* 'efecto vacuna', 'efecto astrazeneca', etc.\n",
    "* 'reacción vacuna'\n",
    "* 'vacuna #AstraZeneca', 'vacuna #Sputnik', 'vacuna #j&j', etc.\n",
    "* sputnik v, moderna\n",
    "* 'vacuna sintoma'\n",
    "* etc.\n",
    "\n",
    "La búsqueda será de los años 2020, 2021 y 2022, y que serán almacenados en un archivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitud = '19.309917325231165'   \n",
    "longitud = '-99.12243737329997'\n",
    "#radio de distancia a partir de 4\n",
    "radio = '31.28km' \n",
    "\n",
    "localizacion = latitud + ',' + longitud + ',' + radio\n",
    "\n",
    "#tweet_count = 100\n",
    "\n",
    "#df_coord = pd.DataFrame(itertools.islice(sntwitter.TwitterSearchScraper(\n",
    "    #'efecto vacuna since:2022-01-01 until:2022-01-31 geocode:\"{}\"'.format(localizacion)).get_items(),tweet_count))[['id','url','date','content','likeCount','retweetCount']]\n",
    "\n",
    "df_coord = pd.DataFrame((sntwitter.TwitterSearchScraper(\n",
    "    'efecto sputnik since:2020-12-24 until:2022-06-17 geocode:\"{}\"'.format(localizacion)).get_items()))[['id','url','date','content','likeCount','retweetCount','replyCount']]\n",
    "\n",
    "df_coord['latitud'] = latitud\n",
    "df_coord['longitud'] = longitud\n",
    "df_coord['radio'] = radio\n",
    "\n",
    "df_coord.to_csv('efectos_covid.csv', sep=',', index=False,mode=\"w+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "!pip install emoji\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install unidecode\n",
    "from unidecode import unidecode\n",
    "\n",
    "import nltk \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "!pip install pyspellchecker\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import spacy\n",
    "!python -m spacy download es\n",
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tw = pd.read_csv(\"post_vacuna.csv\")\n",
    "df_tw.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remover_links(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)   # quitar http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet)  # quitar bitly links\n",
    "    tweet = tweet.strip('[link]')   # quitar [links]\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def remover_menciones(tweet):\n",
    "    tweet = re.sub(r\"@\\S+ \", \"\", tweet)\n",
    "    \n",
    "    return tweet\n",
    "    \n",
    "def remover_hashtag(tweet):  \n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    return tweet\n",
    "    \n",
    "def remover_estilos(tweet):  \n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def remover_numeros(tweet):\n",
    "    tweet = re.sub(r'[0-9]', '', tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def minusculas(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def remover_puntuaciones(tweet):\n",
    "    puntuaciones = '!\"$%&\\'()*+,-./:;<=>¿?[\\\\]^_`{|}~•@º'\n",
    "    tweet = re.sub('[' + puntuaciones + ']+', ' ', tweet) \n",
    "    \n",
    "    #Remover comillas\n",
    "    tweet = tweet.replace(\"’\", \" \")\n",
    "    tweet = tweet.replace(\"“\", '')\n",
    "    tweet = tweet.replace(\"”\", '')\n",
    "    \n",
    "    #Remover puntos de suspensión\n",
    "    tweet = tweet.replace('\\u2026', '')\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def remover_espacios(tweet):\n",
    "    tweet = re.sub('\\s+', ' ', tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def remover_emojis(tweet):\n",
    "    tweet = re.sub(emoji.get_emoji_regexp(), r\"\", tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def remover_letras_repetidas(tweet):\n",
    "    tweet = re.sub(r'(.)\\1{2,}',r'\\1', tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def remover_signos_diacríticos(tweet):\n",
    "    #Conversión de la letra ñ a n\n",
    "    tweet = re.sub(u\"[ñ]\", 'n', tweet)\n",
    "    \n",
    "    #Remover signos diacríticos\n",
    "    tweet = unidecode(tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def tokenizar(tweet):\n",
    "    tk = TweetTokenizer()\n",
    "    tweet_tokenizado = tk.tokenize(tweet) \n",
    "    \n",
    "    return tweet_tokenizado\n",
    "\n",
    "def remover_stopwords(tweet_tokenizado):   \n",
    "    stop_words = stopwords.words('spanish')\n",
    "    tweet_tokenizado = [palabra for palabra in tweet_tokenizado if palabra not in stop_words]\n",
    "    \n",
    "    return tweet_tokenizado\n",
    "\n",
    "def remover_único_token(tweet_tokenizado):\n",
    "    a=list(tweet_tokenizado)\n",
    "    \n",
    "    for palabra in a:\n",
    "        if len(palabra) == 1 :\n",
    "            tweet_tokenizado.remove(palabra)\n",
    "            \n",
    "    return tweet_tokenizado\n",
    "\n",
    "def corrector_otográfico(tweet_tokenizado):\n",
    "    spell = SpellChecker(language='es')\n",
    "    \n",
    "    k = 0\n",
    "    \n",
    "    for palabra in tweet_tokenizado:\n",
    "        palabra_correcta = spell.correction(palabra)\n",
    "        tweet_tokenizado[k] = palabra_correcta\n",
    "        \n",
    "        k+=1 \n",
    "        \n",
    "    return tweet_tokenizado\n",
    "\n",
    "def lista_a_cadena(s):\n",
    "    str1 = \" \"\n",
    "   \n",
    "    return (str1.join(s))\n",
    "\n",
    "def lematizar(tweet_tokenizado):\n",
    "    doc = nlp(tweet_tokenizado)\n",
    "    \n",
    "    lemas = [tok.lemma_.lower() for tok in doc]\n",
    "    \n",
    "    return lemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = df_tw['content']\n",
    "data = []\n",
    "\n",
    "for i in range(len(content)):\n",
    "    tweet = content[i]\n",
    "    tweet = remover_links(tweet)\n",
    "    tweet = remover_menciones(tweet)\n",
    "    tweet = remover_hashtag(tweet)\n",
    "    tweet = remover_estilos(tweet)\n",
    "    tweet = remover_signos_diacríticos(tweet)\n",
    "    tweet = remover_numeros(tweet)\n",
    "    tweet = minusculas(tweet)\n",
    "    tweet = remover_puntuaciones(tweet)\n",
    "    tweet = remover_espacios(tweet)\n",
    "    tweet = remover_emojis(tweet)\n",
    "    tweet = remover_letras_repetidas(tweet)\n",
    "    tweet_tokenizado = tokenizar(tweet)\n",
    "    tweet_tokenizado = remover_stopwords(tweet_tokenizado)\n",
    "    tweet_tokenizado = lista_a_cadena(tweet_tokenizado)\n",
    "    tweet_tokenizado = lematizar(tweet_tokenizado)\n",
    "    tweet_tokenizado = remover_único_token(tweet_tokenizado)\n",
    "    print(tweet_tokenizado)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
